# 深入理解Batch Normalization
## 1.基础知识
* <font color=#0099FF>1.IID独立同分布假设</font>：假设训练数据和测试数据是满足相同分布。 (保障使用训练数据集训练的模型适用于测试集)
* <font color=#0099ff  >  2.Internal Covariate Shift</font>：假设x是属于特征空间的某一样本点，y是标签。covariate这个词，其实就是指这里的x，那么covariate shift可以直接根据字面意思去理解：样本点x的变化。 而Internal的意思的内部，也就是深层网络中的隐藏层。<font color=#8470FF  >所以，Internal Covariate Shift 的意思是在深度神经网络中的各层参数在训练过程中会出现输入的分布不一致，即发生分布变化，也就不满足IID独立同分布假设。</font> 
* <font color=#0099ff  >3.白化 </font>：白化的目的是去除输入数据的冗余信息。假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的。<font color=#8470FF  >白化分为PCA白化和ZCA白化。PCA白化相当于先降维然后再做归一化，而ZCA白化则在PCA的基础上做一个使数据更接近原始数据的处理，然后再做归一化。归一化使把输入数据分布到均值0，方差为1的正态分布。</font> 经过白化处理后的数据有如下两个特征：1.特征之间相关性较低 2.所有特征具有相同的方差。
* <font color=#0099ff>4.标准正态分布</font>:均值为0，标准差为1的正态分布。<font color=#8470FF  >在权重初始化的时候用到标准正态分布，因为只有使用标准正态分布进行权重的初始化才能保证经过激活函数后的值处于神经元不饱和的状态，因为如果处于饱和状态会使梯度下降学习很缓慢，最终会导致反向传播无法正常进行。</font> 
##  2.Batch Normalization的本质思想
### (1) 为什么要引入BN这个方法？解决什么问题而诞生的？

- 之所以提出这个方法是人们发现BP神经网络存在<font color=#8470FF  >梯度消失</font> 的现象，如果存在梯度消失的化会对结果造成错误，而梯度消失的本质原因是因为正向传播的过程中给激活函数输入的值处于激活函数的饱和区，这样的情况会导致求导后的值趋向于0，这就导致梯度的消失。
- 那么我们就要思考<font color=#8470FF  >为什么输入给激活函数的值会处于饱和区</font> 呢？很显然是因为输入的值要么是很大的正数要么是很大的负数，因此我们要想办法把这个值集中在激活函数的非饱和区域，也就是尽量靠中间。于是，论文的作者就想到标准正态分布。所谓的标准正态分布就是均值为0，方差为1的分布，这样的分布可以确保95%的值落在[-2,2]之间，这也就满足输入值落在激活函数的非饱和区，从而初步解决了梯度消失的问题。
- 但是仔细观察会发现，这样的区域非常相似线性函数，那就可能起不到非线性函数的效果，于是作者引入了两个参数scale和shift，通过y=scale*x+shift来左移或者右移一点点并拉伸或者压缩一点点标准正态分布，这样就可以避免类似线性函数。通过这两步的操作就可以解决梯度消失的问题，同时还加快了速度。

### (2) BN的本质思想总结

对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。
