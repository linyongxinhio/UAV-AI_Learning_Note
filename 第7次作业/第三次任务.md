## 完成上次未完成作业  
## task1 完成batch normalization示例小脚本（普通前馈网络）  
* 步骤1 采用MNIST数据集
* 步骤2 采用tf.layers.batch_normalization这种高层函数直接实现
* 步骤3 在不同的超参数条件下验证batch—norm的优势
* 步骤4 改用 tf.nn.batch_normalization 实现底层逻辑，注意总体样本统计和batch样本统计的区别


## task2 完成batch normalization卷积网络脚本
根据上面的示例脚本完成对batch_norm的移植，这里的卷积网络比较简单粗糙，主要为了验证BN层的功能


## task3 采用mlp实现简单自编码器功能

## task4 采用卷积神经网络实现自编码器和图片降噪
思考自编码器为何能实现降噪？最小化重构误差？